# FGSM Attack on MNIST Classifier

This project demonstrates the Fast Gradient Sign Method (FGSM) attack on a Convolutional Neural Network (CNN) trained on the MNIST dataset.

## Project Structure

- `Victim Model.py`: Defines, trains, and evaluates the target CNN model on the MNIST dataset.
- `fgsm_attack.py`: Loads the trained victim model, implements the FGSM attack, generates adversarial examples, evaluates the attack success rate, and visualizes clean vs. adversarial images.

## Requirements

- Python 3.x
- TensorFlow
- NumPy
- Matplotlib

## Setup

1.  **Clone the repository (if applicable):**
    ```bash
    git clone <https://github.com/tydel4/fgsm-ai-model-cybersecurity-attack>
    cd fgsm-ai-model-cybersecurity-attack
    ```
2.  **Install dependencies:**
    ```bash
    pip install tensorflow numpy matplotlib
    ```

## Usage

1.  **Train the Victim Model (Optional but recommended first):**
    Run the `Victim Model.py` script to train the CNN model. This will save the trained model weights implicitly if you modify the script to save, or you can rely on the `fgsm_attack.py` script which retrains it.
    ```bash
    python "Victim Model.py"
    ```
2.  **Run the FGSM Attack:**
    Execute the `fgsm_attack.py` script. This script will:
    - Define and train the victim model (or load a pre-trained one if modified).
    - Evaluate the model's accuracy on clean test data.
    - Implement the FGSM attack.
    - Generate adversarial examples.
    - Evaluate the attack success rate.
    - Display and save comparison plots of clean and adversarial images (`adversarial_examples.png`).
    - Display and save the training/validation accuracy plot (`training_accuracy.png`).
    ```bash
    python fgsm_attack.py
    ```

## Output

- The script will print:
    - Training time.
    - Test accuracy on clean data.
    - Attack time.
    - FGSM attack success rate for the specified epsilon.
- Two plots will be generated and saved:
    - `training_accuracy.png`: Shows the model's training and validation accuracy over epochs.
    - `adversarial_examples.png`: Shows a comparison of original MNIST images and their corresponding adversarial examples generated by the FGSM attack.

## FGSM Attack

The Fast Gradient Sign Method is an adversarial attack designed to fool machine learning models. It works by adding a small perturbation to the input image, calculated based on the gradient of the loss function with respect to the input. The perturbation is designed to maximize the loss, thereby increasing the chance of misclassification.

The perturbation is calculated as:
`perturbation = epsilon * sign(gradient(loss, input_image))`

Where `epsilon` controls the magnitude of the perturbation.
